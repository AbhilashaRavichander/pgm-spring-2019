# Lecture 8 : Causal Discovery and Inference
Causation implies correlation(or dependence) but correlation doesn't imply causation. Consider the case of chocolate consumption in a country compared with the number of nobel laureates in the country

<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-08/scribe1.jpg' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>Chocolate consumptions vs Nobel Laureate</strong>
  </figcaption>
</figure>

From this plot, we can see that they are highly correlated. But it is absurd to say that if we want to increase the number of nobel laureates in a country, we need to eat more chocolate. This is where the idea of causality comes in. They are correlated but chocolate consumption is not the cause of nobel laureates. The following equations capture this difference mathematically

X and Y are dependent if ad only if

$$ \exists x1\neq x2 P(Y|X=x_1) \neq P(Y|X=x_2) $$

X is a cause of Y if and only if

$$  \exists x1\neq x2 P(Y|do(X=x_1) \neq P(Y|X=x_2) $$

The definition requires just a pair of distinct x1, x2 to have different conditional distribution of Y. Y might have same conditional distribution over a range of x values. But if we find even a single pair for which the conditional distribution is different, X and Y are dependent. Same goes for causation 

The causation definition is circular in the sense that if we don't know the causal relation of the other variables, we can't define the intervention of X and thus can't find the causal relation of X and Y. So to define one causal relation, we need all other causal relations. 

## Causal Thinking
### Simpson's Paradox
Let's say there are two type of patients, one with small kidney stones and other with large kidney stones. And there are two types of treatments to choose from, A and B. From the data we can see that, if we consider the patients separately, in both the cases treatment A is better due to higher recovery rate. But if we combine both the categories of patients, we see that the treatment B has higher recovery rate. So, as a doctor, what treatment would you suggest to a patient.

<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-08/scribe2.png' | relative_url }}" />
    </div>
     <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-08/scribe3.png' | relative_url }}" />
    </div>
  </div>
</figure>

Consider another example as follows. We plot the graph of cholesterol vs exercise of people for different age groups. And for each age group, we see that the more you exercise, the less cholesterol you have. But if we combine all the plots (of different age groups), we see the opposite result. The graph says that the more you exercise, the more cholesterol you have.

<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-08/scribe4.png' | relative_url }}" />
    </div>
    <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-08/scribe5.png' | relative_url }}" />
    </div>
  </div>
</figure>

In both the cases, there was a common cause between the quantities we were comparing, and due to this common cause, the relationship of the quantities can be very arbitrary. So, we can't rely on them for any kind of recommendation. We should fix the value of the common cause, and then see the relation between the two quantities we want.

### Strange Dependence
Example - If we go back 50 years, we observe that female college students are smarter than male student on average. 
Gender and IQ are two very different quantities, and we can safely say that they are independent. But given a common cause, like say going to college, now they become dependent. A simple way to understand this is as follows. Consider two independent variable X and Y. But if we are given the sum of X and Y, now they both are dependent.
<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-08/scribe6.png' | relative_url }}" />
    </div>
  </div>
</figure>

#### Monty Hall problem
The setup is as follows. There are 3 doors, behind one of which there is 1000\$. If we select that door, we get the prize money. So we make our initial guess, then the game show host opens one of the empty doors and shows there is no money behind that door. Now should we stick with our initial choice or switch to the other door.

The answer is to switch our choice. If we switch, our probabilty of winning becomes 2/3 rather than the 1/3 if we stick to our choice. We can think of it as follows. There are three options to select from initially, two empty doors and one with the prize money. If we are always switching, we will end up with the prize money if we have initially chosen the empty door and we will lose if we have initially selected the door with the prize money. So to win, we need to select the empty door at first whose probability is 2/3. So the chance of us winning the car if we switch is 2/3.

### Causal Models
If we have causal models, it is very easy to do intervention. We just need to delete all the incoming edges into the nodes which we are intervening and then do inference on the obtained graphical structure.

Also if we know the causal structure, we can apply some of the known distributions into new scenarios. This idea is used in transfer learning.

### Prediction
This answers the question of $P(Y | X)$ which means the probability of Y if X happens to take a particular value. This can be found out just from the observational data. We don't need any graphs, or causal realtions to find this value. 

### Intervention
This answers the question of $P(Y | do(X))$ which means what is the probability of Y if X is set to a particular value keeping all the other variables same. For this, we definitely need to know all the causal relations of all the variables.

### Counterfactual Thinking
??

## Identification of Causal Relation

### Causal DAGs
Each edge in a causal DAG corresponds to direct causal influence between the nodes and it is asymmetric.
Let $P_x(V)$ be the distribution resulting from intervention do(X=x). Then a DAG is a causal DAG if
\begin{itemize}
    \item $P_x(V)$ is Markov relative to G. This distribution is still factorizable according to the Markov property (product of proabbility of each node given its parents) with respect to G
    
    \item $P_x(V_i = v_i) = 1$ for $V_i \in X$ and $v_i$ consistent with X=x. This is so because we have performed intervention on X and we have set that to x. Hence we know those variables with absolute certainty
    
    \item $P_x(V_i | PA_i) = P(V_i | PA_i)$ for all $V_i \notin X$ where $PA_i$ represents the parents of $V_i$. That means the conditional distributions of all the nodes not included in the intervention remains the same before and after the intervention. We can see this graphically since while doing intervention, we just delete all the incoming edges into the nodes on which intervention is performed. So the other nodes remain unchanges(in terms of parents)
\end{itemize}

### Randomized Control Experiments
We change a controlled variable and observe the change in an outcome variable. All the other variables that influence the outcome variable is either fixed or varied at random. So now any change observed in the outcome variable must have been due to the change in the controlled variable.

Its disadvantage is that its expensive and in many cases impossible to do. The outcome variable can depend on many other variables. So, to find one causal relation, we would need a lot of data and we need to ensure that the other variables are fixed or varied randomly across the experiments which is a very hard task.

### Kidney Stone Experiment
Lets revisit the kidney stone treatment example. Let R = Recovery, T = Treatment and S = Stone size, then
$$ P(R|T) = \sum_S P(R|S,T)P(S|T) $$
$$ P(R|do(T)) = \sum_S P(R|S,T)P(S) $$

Note that while intervening, we cut off all incoming edges into the treatment node. So, now S and T are independent
Figures

### Causal Effect Identifiability
An aspect of a statistical model is identifiable, if it cannot be changed without there also being some change in the distribution of the observed variables. So if we can express a quantity as a function of the distributions of observed variables, then we can say that quantity is identifiable. Thus the causal effect is identifiable if and only if we can represent $P(Y|do(X))$ in terms of the distribution of observed variables. i.e. if

$P_{M1}(Y|do(X)) = P_{M2}(Y|do(X))$ for every pair of models M1 and M2 with the dame graph and the same distibution of observed variables(Unobserved / Latent variable distributions can be changed)

Example - If we have just two random variables X and Y(both observed) where X is the cause of Y and there is no confounder.So, $P(Y|do(X)) = P(Y|X)$ and we know this is identifiable because for changing $P(Y|X)$, we would have to change the distribution of P(X,Y).

% But, now lets say we have a confounder C, which is the common cause of both X and Y. And this is not observed. Now, we can change the distribution of C such that P(X,Y) is unchanged but we get different $P(Y|do(X))$\\
$P(Y|do(X))$ can also be written as $P(y | \hat{x})$
#### Back Door Criterion 
A set of variables Z satisfies the back-door criterion relative to an ordered pair of variables $(X_i, X_j)$ if
\begin{itemize}
    \item No node in Z is a descendant of $X_i$
    \item Z blocks every path between $X_i$ and $X_j$ that contains an arrow into $X_i$ (Hence the name back-door)
\end{itemize}

If the set of nodes Z follows these two conditions then \[P (y | \hat{x}) = \sum_z P(y|x,z)P(z) \]
Same as we saw earlier in the case of kidney stone treatment. And thus $P(y|\hat{x})$ is now identifiable because of this back-door adjustment.\\
Proof : left
#### Front Door Criterion
A set of variables Z satisfies the front-door criterion relative to an ordered pair of variables $(X_i, X_j)$ if
\begin{itemize}
    \item Z intercept all directed paths from X to Y (hence the name front-door)
    \item Z There is no back-door path from X to Z 
    \item All back-door paths from Z to Y are blocked by X
\end{itemize}
If the set of nodes Z follows these two conditions then \[P (y | \hat{x}) = \sum_z P(z|x)\sum_{x'}P(y|x',z)P(x') \]
Proof: left
#### Propensity Score
### Causal Discovery
